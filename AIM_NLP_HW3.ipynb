{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvhPa7a59AIG"
   },
   "source": [
    "<font color=red>**Danger zone:**</font> you'll be fine-tuning a model to generate positive, negative or even toxic reviews. We'll be doing this for fun, but this is also the technique for [review bombing](https://en.wikipedia.org/wiki/Review_bomb), bot farms on social media and other less than dignified stuff. It is ultimately your decision how you apply this knowledge, but before you choose, ask yourself: is this why you chose to learn ML?\n",
    "\n",
    "\n",
    "# LLMs Alignment with Reinforcement Learning from human feedback (RLHF).\n",
    "\n",
    "_based on the [original notebook](https://github.com/antndlcrx/oxford-llms-workshop/blob/main/materials/seminars/day_3/8_LLMs%20alignment%20with%20RLHF.ipynb) by Ilya Boytsov for the Oxford LLMs workshop_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgfL4bSSAXan"
   },
   "source": [
    "In this session, you're gonna fine-tune a language model with reinforcement learning to make it generate good (or bad) reviews.\n",
    "\n",
    "To perform RL-based fine-tuning, we'll use a new (in this course) library called [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl). TRL implements the main reinforcement learning components of RLHF: reward modeling and fine-tuning with PPO.\n",
    "\n",
    "![img](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/TRL-readme.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uADkArNHQDW6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %pip install -q trl==0.7.4 transformers==4.33.1 datasets==2.15.0 peft==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -atasets (/home/nikolaev.oleg15/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -atasets (/home/nikolaev.oleg15/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: gcsfs==2024.5.0 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (2024.5.0)\n",
      "Requirement already satisfied: fsspec==2024.5.0 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (2024.5.0)\n",
      "Requirement already satisfied: trl==0.8.6 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (0.8.6)\n",
      "Requirement already satisfied: transformers==4.44.0 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (4.44.0)\n",
      "Collecting datasets==2.20.0\n",
      "  Using cached datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "Requirement already satisfied: peft==0.12.0 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (0.12.0)\n",
      "Requirement already satisfied: decorator>4.1.2 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from gcsfs==2024.5.0) (5.1.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from gcsfs==2024.5.0) (3.11.2)\n",
      "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs==2024.5.0) (2.37.0)\n",
      "Requirement already satisfied: google-cloud-storage in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from gcsfs==2024.5.0) (2.19.0)\n",
      "Requirement already satisfied: requests in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from gcsfs==2024.5.0) (2.32.3)\n",
      "Requirement already satisfied: google-auth-oauthlib in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from gcsfs==2024.5.0) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from trl==0.8.6) (1.24.4)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from trl==0.8.6) (2.5.1)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from trl==0.8.6) (0.9.2)\n",
      "Requirement already satisfied: accelerate in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from trl==0.8.6) (1.1.1)\n",
      "Requirement already satisfied: filelock in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from transformers==4.44.0) (3.16.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from transformers==4.44.0) (0.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from transformers==4.44.0) (0.26.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from transformers==4.44.0) (4.67.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from transformers==4.44.0) (0.4.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from transformers==4.44.0) (2024.9.11)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from transformers==4.44.0) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from transformers==4.44.0) (23.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from datasets==2.20.0) (18.0.0)\n",
      "Requirement already satisfied: xxhash in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from datasets==2.20.0) (3.5.0)\n",
      "Requirement already satisfied: pandas in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from datasets==2.20.0) (1.5.3)\n",
      "Requirement already satisfied: multiprocess in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from datasets==2.20.0) (0.70.15)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from datasets==2.20.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from datasets==2.20.0) (0.3.7)\n",
      "Requirement already satisfied: psutil in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from peft==0.12.0) (5.9.8)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.5.0) (2.4.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.5.0) (1.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.5.0) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.5.0) (1.17.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.5.0) (5.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.5.0) (6.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.5.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.5.0) (23.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs==2024.5.0) (0.4.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs==2024.5.0) (5.3.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs==2024.5.0) (4.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.0) (4.10.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from requests->gcsfs==2024.5.0) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from requests->gcsfs==2024.5.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from requests->gcsfs==2024.5.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from requests->gcsfs==2024.5.0) (2024.2.2)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (3.1.0)\n",
      "Requirement already satisfied: networkx in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (3.3)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (2.21.5)\n",
      "Requirement already satisfied: jinja2 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.4.0->trl==0.8.6) (1.3.0)\n",
      "Requirement already satisfied: rich>=11.1.0 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6) (13.7.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6) (1.7.1)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6) (0.16)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6) (4.4.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib->gcsfs==2024.5.0) (2.0.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from google-cloud-storage->gcsfs==2024.5.0) (1.6.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from google-cloud-storage->gcsfs==2024.5.0) (2.4.1)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from google-cloud-storage->gcsfs==2024.5.0) (2.24.0)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from google-cloud-storage->gcsfs==2024.5.0) (2.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from pandas->datasets==2.20.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from pandas->datasets==2.20.0) (2023.3.post1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs==2024.5.0) (1.66.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs==2024.5.0) (1.25.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs==2024.5.0) (4.25.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs==2024.5.0) (0.6.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.20.0) (1.16.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==2024.5.0) (3.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (2.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl==0.8.6) (2.1.5)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (0.1.2)\n",
      "\u001b[33mWARNING: No metadata found in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: No metadata found in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -atasets (/home/nikolaev.oleg15/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing requirements for datasets: [Errno 2] No such file or directory: '/home/nikolaev.oleg15/.local/lib/python3.10/site-packages/datasets-3.1.0.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Error while checking for conflicts. Please file an issue on pip's issue tracker: https://github.com/pypa/pip/issues/new\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n",
      "    return self.__dep_map\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _DistInfoDistribution__dep_map\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3012, in _parsed_pkg_info\n",
      "    return self._pkg_info\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _pkg_info\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/commands/install.py\", line 529, in _determine_conflicts\n",
      "    return check_install_conflicts(to_install)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
      "    package_set, _ = create_package_set_from_installed()\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
      "    dependencies = list(dist.iter_dependencies())\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 202, in iter_dependencies\n",
      "    return self._dist.requires(extras)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n",
      "    dm = self._dep_map\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n",
      "    self.__dep_map = self._compute_dependencies()\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3032, in _compute_dependencies\n",
      "    for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3014, in _parsed_pkg_info\n",
      "    metadata = self.get_metadata(self.PKG_INFO)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 1895, in get_metadata\n",
      "    raise KeyError(\"No metadata except PKG-INFO is available\")\n",
      "KeyError: 'No metadata except PKG-INFO is available'\u001b[0m\u001b[31m\n",
      "\u001b[0mInstalling collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -atasets (/home/nikolaev.oleg15/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: No metadata found in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: datasets 3.1.0\n",
      "\u001b[33m    WARNING: No metadata found in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: No metadata found in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: No metadata found in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: No metadata found in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: No metadata found in /home/nikolaev.oleg15/.local/lib/python3.10/site-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Cannot uninstall datasets 3.1.0, RECORD file not found. You might be able to recover from this via: 'pip install --force-reinstall --no-deps datasets==3.1.0'.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install  gcsfs==2024.5.0 fsspec==2024.5.0 trl==0.8.6 transformers==4.44.0 datasets==2.20.0 peft==0.12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cJfrTbFYAx8"
   },
   "source": [
    "### Tutorial: align the model to generate positive movie reviews\n",
    "\n",
    "To see how TRL works, we'll use it to align GPT2 on IMDB dataset to generate positive (or negative) movie reviews. In fact, __it's your choice whether you want positive or negative reviews.__\n",
    "\n",
    "But before you choose, let's take a look at the baseline model: a GPT-2 fine-tuned on generating arbitrary movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "pHs22MXdPify"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikolaev.oleg15/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "main_tokenizer = transformers.AutoTokenizer.from_pretrained(\"lvwerra/gpt2-imdb\")\n",
    "main_model = transformers.AutoModelForCausalLM.from_pretrained(\"lvwerra/gpt2-imdb\", device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "KE3jo7uhQrvK",
    "outputId": "6ae43c17-7ecc-4db7-c7c8-1e4975c621b3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text: The movie has some fantastic scenes, like when a car breaks down at an intersection, you start to see this and think to yourself \"Wow, it's happening.\" Then the car slows down in this scene so you realize the guy in the movie was supposed to\n"
     ]
    }
   ],
   "source": [
    "inputs = main_tokenizer(\"The movie\", return_tensors='pt').to(device)\n",
    "generated_ids = main_model.generate(**inputs, max_new_tokens=50, do_sample=True)\n",
    "print(\"\\nGenerated text:\", main_tokenizer.decode(generated_ids.flatten().cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJbfhMEpR4Sz"
   },
   "source": [
    "If you run this cell a couple of times, you'll see that the model generates both positive, negative and neutral reviews in some proportion. What we're gonna do next is teach the model to generate more positive (or negative) reviews.\n",
    "\n",
    "Similarly to InstructGPT, we're gonna do that in 2 stages:\n",
    "- **train a reward model** to assign higher values to positive (or negative) reviews\n",
    "- fine-tune the language model to **maximize that reward using [proximal policy optimization](https://openai.com/research/openai-baselines-ppo)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bcv4uC7xb26Z"
   },
   "source": [
    "## Stage 1: train a reward model\n",
    "\n",
    "First, we'll train a BERT-like model as our reward model. We'll generate a synthetic pairwise rankings to emulate human rankings.\n",
    "\n",
    "__Q:__ why do I need a reward model? Can I just use a pre-trained sentiment classifier? <br> __A:__ Yes, you can - but that only works for movie reviews. But this tutorial will teach you how to do RLHF for any kind objective.\n",
    "\n",
    "\n",
    "__If you actually want to maximize sentiment (or other \"label\") instead of human preferences, train reward model as a classifier! (see week5)__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "WeOdZ_ayc9dy",
    "outputId": "0dd54557-4237-4a30-d1b9-0ca00d7a7d04"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# We'll be fine-tuning a small BERT-like model for now. Please try other models for the main assignment.\n",
    "reward_model = transformers.AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-cased\", device_map=device)\n",
    "reward_tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZUUNQo-d11b"
   },
   "source": [
    "__Note that__ the reward model has a separate tokenizer, different from the main model. They don't need to be the same for RLHF fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "TTWR-48ZXQX6"
   },
   "outputs": [],
   "source": [
    "# To train a reward model, you need a dataset (or generator) of positive-negative pairs.\n",
    "# Each training sample should be a dict with 4 keys:\n",
    "#  - input_ids_chosen, attention_mask_chosen = tokenizer(\"A sentence that human labeler likes more\")\n",
    "#  - input_ids_rejected, attention_mask_rejected = tokenizer(\"A sentence that human labeler likes less\")\n",
    "\n",
    "import torch\n",
    "import datasets\n",
    "\n",
    "class IMDBPairwiseDataset(torch.utils.data.Dataset):\n",
    "    \"\"\" A dataset of all possible pairs of chosen and texts in TRT reward training format \"\"\"\n",
    "    def __init__(self, imdb, tokenizer, accepted_label: int):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.chosen_texts = [row['text'] for row in imdb if row['label'] == accepted_label]\n",
    "        self.rejected_texts = [row['text'] for row in imdb if row['label'] != accepted_label]\n",
    "        assert self.chosen_texts, f\"no texts with label {accepted_label}\"\n",
    "        print(f\"Found {len(self.chosen_texts)} chosen and {len(self.rejected_texts)} rejected texts, {len(self)} pairs\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chosen_texts) * len(self.rejected_texts)  # all pairs\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        chosen = self.tokenizer(self.chosen_texts[index // len(self.chosen_texts)], truncation=True)\n",
    "        rejected = self.tokenizer(self.rejected_texts[index % len(self.chosen_texts)], truncation=True)\n",
    "        return dict(input_ids_chosen=chosen['input_ids'], attention_mask_chosen=chosen['attention_mask'],\n",
    "                    input_ids_rejected=rejected['input_ids'], attention_mask_rejected=rejected['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "olo-bvgNcwEC",
    "outputId": "16051d61-c450-4a3e-8689-f91f28f8280e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 chosen and 12500 rejected texts, 156250000 pairs\n",
      "CHOSEN: [CLS] Lars Von Trier is never backward in trying out new techniques. Some of them are very original while others are best forgotten. < br / > < br / > He depicts postwar Germany as a nightmarish train journey. With so many cities lying in ruins, Leo Kessler a young American of German descent feels obliged to help in their restoration. It is not a simple task as he quickly finds out. < br / > < br / > His uncle finds him a job as a night conductor on the Zentropa Railway Line. His job is to attend to the needs of the passengers. When the shoes are polished a chalk mark is made on the soles. A terrible argument ensues when a passenger's shoes are not chalked despite the fact they have been polished. There are many allusions to the German fanaticism of adherence to such stupid details. < br / > < br / > The railway journey is like an allegory representing man's procession through life with all its trials and tribulations. In one sequence Leo dashes through the back carriages to discover them filled with half - starved bodies appearing to have just escaped from Auschwitz. These images, horrible as they are, are fleeting as in a dream, each with its own terrible impact yet unconnected. < br / > < br / > At a station called Urmitz Leo jumps from the train with a parceled bomb. In view of many by - standers he connects the bomb to the underside of a carriage. He returns to his cabin and makes a connection to a time clock. Later he jumps from the train ( at high speed ) and lies in the cool grass on a river bank. Looking at the stars above he decides that his job is to build and not destroy. Subsequently as he sees the train approaching a giant bridge he runs at breakneck speed to board the train and stop the clock. If you care to analyse the situation it is a completely impossible task. Quite ridiculous in fact. It could only happen in a dream. < br / > < br / > It's strange how one remembers little details such as a row of cups hanging on hooks and rattling away with the swaying of the train. < br / > < br / > Despite the fact that this film is widely acclaimed, I prefer Lars Von Trier's later films ( Breaking the Waves and The Idiots ). The bomb scene described above really put me off. Perhaps I'm a [SEP]\n",
      "REJECTED: [CLS] I saw this movie in 1979, I was 17 or 18, when it was released. The theater was perhaps 1 / 4 full when the movie started. Ten minutes into the movie me and the friend who went with me to see the film were the only two people in the theater. The movie was really weird and had no plot or reason to its script and people demanded their money back. We decided to stay for the ENTIRE movie.... why endure such torture??... here's why. We wanted to be true movie critics... to have a standard to base all other movies on it is hard to justify saying you have seen the best movie ( a 10 ) they always come up with something better. But, it is easy to be able to base all other movies off of the worst movie ever made ( and this is it... a 1 at best ). There may be other movies out there that truly qualify as a 1, but I have yet to see them. I now base all movies I see on a scale based on this worst.... I AM A TRUE MOVIE CRITIC... he he. [SEP]\n"
     ]
    }
   ],
   "source": [
    "TARGET_LABEL = 1   # and make sure it works by reviewing the sample printed below\n",
    "imdb = datasets.load_dataset(\"imdb\", split='train')\n",
    "reward_data = IMDBPairwiseDataset(imdb, reward_tokenizer, accepted_label=TARGET_LABEL)\n",
    "\n",
    "sample = reward_data[31337]\n",
    "print('CHOSEN:', reward_tokenizer.decode(sample['input_ids_chosen']))\n",
    "print('REJECTED:', reward_tokenizer.decode(sample['input_ids_rejected']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZRczyofiSl0"
   },
   "source": [
    "We'll be using `trl.RewardTrainer` - a special case of `transformers.Trainer` that you used in the past. `RewardTrainer` accepts the same format of training arguments (e.g. batch size, gradient checkpointing) as before, except that it trains the model for the pairwise reward objective from [the InstructGPT paper](https://arxiv.org/pdf/2203.02155.pdf):\n",
    "\n",
    "![img](https://i.imgur.com/2JzNAPs.png)\n",
    "\n",
    "Note that the model itself does not score pairs: it processes chosen ($y_w$) and rejected ($y_l$) samples independently. To minimize this loss, the reward model needs to score chosen sample higher than the rejected one. Note that the formula also assumes some context $x$, which is useful for seq2seq tasks. In our case of movie reviews, $x$ is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1053
    },
    "id": "oaQ_-JAzakJs",
    "outputId": "4ffe023f-4773-4a47-8af2-86839db874b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikolaev.oleg15/.local/lib/python3.10/site-packages/trl/trainer/reward_trainer.py:172: UserWarning: When using RewardDataCollatorWithPadding, you should set `max_length` in RewardConfig. It will be set to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/home/nikolaev.oleg15/.local/lib/python3.10/site-packages/trl/trainer/reward_trainer.py:189: UserWarning: When using RewardDataCollatorWithPadding, you should set `remove_unused_columns=False` in your RewardConfig we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/home/nikolaev.oleg15/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 04:31, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.096300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.088500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.098300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.089300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.093800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.075600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.054400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.074000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.061900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.063500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.063400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.051400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.064200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.056900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.056600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.062100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.036500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.039900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikolaev.oleg15/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.06643827652931214, metrics={'train_runtime': 279.5615, 'train_samples_per_second': 114.465, 'train_steps_per_second': 3.577, 'total_flos': 0.0, 'train_loss': 0.06643827652931214, 'epoch': 0.00020479997902848215})"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import trl\n",
    "\n",
    "training_args = trl.RewardConfig(  # like transformers.TrainingArguments\n",
    "    output_dir=\"reward_model\",\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=1.41e-5,\n",
    "    max_steps=1_000,              # note: training may need more than 1k steps\n",
    "    logging_steps=50,\n",
    "    gradient_checkpointing=True,  # reduce memory usage but train ~30% slower\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    fp16=True,                 # disable this on CPU or on very old GPUs\n",
    "    report_to=\"none\"\n",
    "    # you may add any other hyperparameters that you found useful in weeks 5-7\n",
    ")\n",
    "\n",
    "trainer = trl.RewardTrainer(\n",
    "    model=reward_model,\n",
    "    args=training_args,\n",
    "    tokenizer=reward_tokenizer,\n",
    "    train_dataset=reward_data,\n",
    "    peft_config=None,  # optionally, you may tune with LoRA, prompt-tuning, etc\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "CRk7z-2r4C-A",
    "outputId": "5b99e451-e2e7-44bb-cf11-c28a8eb8ae64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_model.gradient_checkpointing_disable()\n",
    "reward_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZIaS-gRo8yc"
   },
   "source": [
    "### Sanity-check the reward model (1 point)\n",
    "\n",
    "Let's check how our reward model performs.\n",
    "\n",
    "__Your task__ is to measure how often does your reward model can rank a pair of (chosen and rejected) reviews correctly. Please measure this separately for train data (`imdb`) and a separate test set loaded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "IeQ108nOZ7nO",
    "outputId": "6de29424-1308-4677-e909-4f3f9d8ceb6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: This movie sucked. It really was a waste of my life. The acting was atrocious, the plot completely implausible. Long, long story short, these people get \"terrorized\" by this pathetic \"crazed killer\", but completely fail to fight back in any manner. And this is after they take a raft on a camping trip, with no gear, and show up at a campsite that is already assembled and completely stocked with food and clothes and the daughters headphones. Additionally, after their boat goes missing, they panic that they're stuck in the woods, but then the daughters boyfriend just shows up and they apparently never consider that they could just hike out of the woods like he did to get to them. Like I said, this movie sucks. A complete joke. Don't let your girlfriend talk you into watching it.\n",
      "REWARD: -4.2421875\n",
      "LABEL: 0\n",
      "\n",
      "TEXT: Good: Engaging cinematic firefights, great presentation, vehicles are actually fun to drive, fairly appealing multiplayer, faithful to the movie, and the list goes on.<br /><br />Bad: Main missions are a bit short.<br /><br />This game defines what a \"good\" third person shooter(not necessarily a spy-game) is. Great firefights carry on the story and make you want to complete EVERY single mission through, and unlock all the genuine bonuses the game has to offer. The hype this game had, was lived up to, and I personally think you should buy it, and hook up with a couple of friends and play this one. Loads of fun. <br /><br />The sound in this game, is a rip-roaring achievement from a few previous bond games, and firing a weapon, really feels like you're firing a weapon. It ties in with the aspect that you are a deadly and ruthless spy.<br /><br />All in all, this game makes you excited and satisfied after you make it through, and some multiplayer that can compete with the standards of the crafty James Bond \"Nightfire\" game for gamecube.\n",
      "REWARD: 6.00390625\n",
      "LABEL: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample_index in 45, 16000:\n",
    "  print('TEXT:', imdb[sample_index]['text'])\n",
    "  inputs = reward_tokenizer(\n",
    "      imdb[sample_index]['text'], truncation=True, return_tensors='pt').to(device)\n",
    "  with torch.no_grad():\n",
    "    reward = reward_model(**inputs).logits[0, 0].item()\n",
    "    print(\"REWARD:\", reward)\n",
    "  print('LABEL:', imdb[sample_index]['label'])\n",
    "  print()\n",
    "\n",
    "# note: your reward model may produce different absolute rewards.\n",
    "# This is fine as long as the rewards are ordered correctly (most of the time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_test = datasets.load_dataset(\"imdb\", split='test')\n",
    "\n",
    "# <a whole lot of your code here, feel free to spit it as you see fit>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 1 - reward положительны, например 4\n",
    "# label 0 - reward отрицательный, например -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_roc_auc(reward_model, dataset, tokenizer, device, N=None):\n",
    "    rewards = []\n",
    "    labels = []\n",
    "    if N:\n",
    "        dataset = dataset.shuffle(seed=42)\n",
    "    for n, sample in enumerate(tqdm(dataset)):\n",
    "        inputs = tokenizer(sample['text'], truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            reward = reward_model(**inputs).logits[0, 0].item()\n",
    "        rewards.append(max(0, reward))\n",
    "        labels.append(sample['label'])\n",
    "        if N and n >= N:\n",
    "            break\n",
    "    rewards = np.array(rewards)\n",
    "    labels = np.array(labels)\n",
    "    # ROC AUC score\n",
    "    roc_auc = roc_auc_score(labels, rewards)\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [01:48<00:00, 231.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC on training data: 0.9723826399999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "roc_auc_train = evaluate_roc_auc(reward_model, imdb, reward_tokenizer, device, N=None)\n",
    "print(f\"ROC AUC on training data: {roc_auc_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [01:47<00:00, 232.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC on test data: 0.9489861792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "roc_auc_test = evaluate_roc_auc(reward_model, imdb_test, reward_tokenizer, device, N=None)\n",
    "print(f\"ROC AUC on test data: {roc_auc_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHCWHMyRw2-k"
   },
   "source": [
    "### Reward-guided generation (1 point)\n",
    "\n",
    "If you did everything right, by now you should have a decent reward model. Before we use it for reinforcement learning, let's see if we can align model samples without any training.\n",
    "\n",
    "To do so, you can use reward-guided inference: __generate N=16 samples, then select the one with the highest reward__ (according to your reward model).\n",
    "\n",
    "For this problem, it's on you to demonstrate whether or not your code works. Find at least 5 neutral prompts such as \"This movie is\" (...), generate samples, rank them based on reward and show which samples get the highest reward.\n",
    "\n",
    "Note: it is faster to generate samples in parallel, rather than sequentially, as follows:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "8BRsyb2cq5dR",
    "outputId": "4d4c917c-4a79-4db8-fff4-a63167256044"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: It was also the first time I'd encountered anything remotely like this. The animation was horrible. Very bland and very annoying, with really poor CG. Even the main characters wore clothes for nothing. The animation was awful. I'd rather make a movie with people\n",
      "Sample: It was really funny but had some very thin characters and some very hard lines that were only intended for the main character. I am not a big fan of gay movies but this one had good lines, but that's it. Don't get me wrong, I\n",
      "Sample: It was the first time that a new musical had been created in my lifetime.. and the only time I know I was ever so surprised when I heard it. The songs were outstanding, and I must agree. One thing I can't say is that i won\n",
      "Sample: It was a great film, and the movie is a wonderful way to begin a movie. The action sequences are intense, but most of the story seems to move at a snail's pace as it spirals along, and at times the actors manage to jump so\n",
      "Sample: It was so good. I love this movie.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "inputs = main_tokenizer([\"It was\"] * 5, return_tensors='pt').to(device)\n",
    "for candidate in main_model.generate(**inputs, max_new_tokens=50, do_sample=True):\n",
    "  print(\"Sample:\", main_tokenizer.decode(candidate.flatten().cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "r08F4lz7yxE1"
   },
   "outputs": [],
   "source": [
    "# <YOUR CODE HERE> - feel free to organize it as you see fit\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "def reward_guided_generation(prompt, num_samples=16, max_new_tokens=50):\n",
    "    inputs = main_tokenizer([prompt] * num_samples, return_tensors='pt').to(device)\n",
    "    generated_ids = main_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_k=50,  # diversity\n",
    "        top_p=0.95,  # diversity\n",
    "        temperature=0.7  # creativity\n",
    "    )\n",
    "    samples = [main_tokenizer.decode(ids, skip_special_tokens=True) for ids in generated_ids]\n",
    "    rewards = []\n",
    "    for sample in samples:\n",
    "        sample_inputs = reward_tokenizer(sample, truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            reward = reward_model(**sample_inputs).logits[0, 0].item()\n",
    "        rewards.append(reward)\n",
    "    best_sample_index = np.argmax(rewards)\n",
    "    best_sample = samples[best_sample_index]\n",
    "    best_reward = rewards[best_sample_index]\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Best Sample: {best_sample}\")\n",
    "    print(f\"Best Reward: {best_reward:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    return best_sample, best_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: This movie is\n",
      "Best Sample: This movie is absolutely worth seeing. The acting is excellent, the cinematography is incredible, and the direction is great. The cast is all talented and talented, and it is truly the best I have ever seen.<br /><br />The movie is full of\n",
      "Best Reward: 8.3281\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Actors were\n",
      "Best Sample: Actors were also very good, the story is simple and entertaining.<br /><br />A lot of what makes this movie interesting is the fact that the characters are all very likable and believable.<br /><br />The acting is very good and the\n",
      "Best Reward: 8.2109\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: I think the move was\n",
      "Best Sample: I think the move was just as effective. The script was so well written, and the acting was so good, that I didn't mind it. The film is so well-written that it is easy to watch. <br /><br />The film is a great\n",
      "Best Reward: 8.0859\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The character was\n",
      "Best Sample: The character was well written and acted and the acting was good. The movie is a very good example of how to use the \"good\" in a movie. You can see the bad guys as they are a lot more interesting than the good ones. I enjoyed the\n",
      "Best Reward: 7.8789\n",
      "--------------------------------------------------\n",
      "Prompt: The performance was\n",
      "Best Sample: The performance was outstanding, and the story was well told. A great movie!\n",
      "Best Reward: 8.1172\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"This movie is\",\n",
    "    \"Actors were\",\n",
    "    \"I think the move was\",\n",
    "    \"The character was\",\n",
    "    \"The performance was\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    reward_guided_generation(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NjQ40BRoH5f"
   },
   "source": [
    "# Stage 2: fine-tune the main model with RL\n",
    "\n",
    "\n",
    "For this tutorial, we will optimize GPT2 to produce positive IMDB movie reviews using the reward model you trained above.\n",
    "\n",
    "Unlike supervised fine-tuning, RL allows model to generate it's own sentences on each training step. Then, it calculates the reward of those specific sentences, and finally, updates the model to increase the probability of sentences with high reward.\n",
    "\n",
    "Thus, each RLHF consists of three stages: __Rollout__, __Evaluation__ and __Update__\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<img src='https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/gpt2_bert_training.png' width='600'>\n",
    "\n",
    "The update stage depends on the specific RL algorithm. We'll be using Proximal Policy Optimization, or [PPO](https://arxiv.org/abs/1707.06347), similarly to what was used for InstructGPT.\n",
    "\n",
    "Before we run those 3 stages, however, we need to create a dataset of \"queries\" - partial reviews in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "f6ba50eafdeb4b94a1e7c22c5027f709",
      "63e252cfd9f44ef79137473b618828dd",
      "4ad4da252cb64e818d35eb3869adc81c",
      "d3a8dcfad53b4de885b39ee83e6029ef",
      "44f3d7c434354a90bfa3d4750048b80f",
      "2ca9e640d5d549658e42fd73af8ea399",
      "c8ea1c2d3b5040378d0f2bd213933603",
      "1692cc1532df4c2695c729a964a31da8",
      "55c624445ca44090a2f109d3bf5f3f6a",
      "411cd47238a94edc8283e178e04d386f",
      "9604bff7c9414071a55d063fdf4174fa"
     ]
    },
    "id": "jm5IUrer0xd_",
    "outputId": "ea58969e-cb80-4ff5-8a34-7c6a2d14cab3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb07bcce69c4702a8f09a49e8ea3cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bacf9ae7c542f4ad85e8b2424a4abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24895 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Note: this code is specific to IMDB; you will need to re-write it for other tasks\n",
    "imdb_for_rlhf = imdb.filter(lambda row: len(row['text']) > 200, batched=False)\n",
    "imdb_for_rlhf = imdb_for_rlhf.remove_columns(['label'])\n",
    "sample_length = trl.core.LengthSampler(2, 8)  # use the first 2-8 tokens as query\n",
    "\n",
    "def select_query_and_tokenize(sample):\n",
    "    query_ids = main_tokenizer.encode(sample[\"text\"])[: sample_length()]\n",
    "    sample[\"query\"] = main_tokenizer.decode(query_ids)  # query is the only required column\n",
    "    sample[\"input_ids\"] = query_ids  # to avoid re-tokenizing later\n",
    "    return sample  # we do not need the rest - it will be generated by the model\n",
    "\n",
    "imdb_for_rlhf = imdb_for_rlhf.map(select_query_and_tokenize, batched=False)\n",
    "imdb_for_rlhf.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKIAyilP3Bf1"
   },
   "source": [
    "Next, let's prepare your reward model to predict rewards on whatever reviews were generated. Note that we use plaintext reviews because main model uses a different tokenizer from the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "kkm4MLOr20Jk"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def compute_reward(texts: List[str]) -> torch.Tensor:\n",
    "  inputs = reward_tokenizer(texts, truncation=True, padding=True, return_tensors='pt').to(device)\n",
    "  with torch.no_grad():\n",
    "    return reward_model(**inputs).logits[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "7wJto13M3vWu",
    "outputId": "0214bd72-21e0-49ea-ec33-12f9e9d587d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.3125,  8.1328], device='cuda:0')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_reward([imdb[45]['text'], imdb[16000]['text']])  # test on human-written reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3buACYV4QLJ"
   },
   "source": [
    "Finally, we move to RL training. In this tutorial, we'll train LoRA adapters and not the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nar1yXgl4KQa",
    "outputId": "0dfa2e71-48ef-497b-90cf-e156fa921c7e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikolaev.oleg15/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,179,648 || all params: 125,620,225 || trainable%: 0.9391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikolaev.oleg15/.local/lib/python3.10/site-packages/trl/models/modeling_base.py:328: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loading_func(filename if not use_safe else safe_filename, **load_kwargs)\n",
      "/home/nikolaev.oleg15/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1091: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import peft\n",
    "peft_config = peft.LoraConfig(\n",
    "    task_type=peft.TaskType.CAUSAL_LM, r=32, lora_alpha=32, lora_dropout=0.0, inference_mode=False\n",
    ")\n",
    "\n",
    "# reload main model as AutoModelForCausalLMWithValueHead - with an extra head needed for PPO\n",
    "main_tokenizer = transformers.AutoTokenizer.from_pretrained(\"lvwerra/gpt2-imdb\")\n",
    "main_tokenizer.pad_token = main_tokenizer.eos_token\n",
    "\n",
    "main_model = trl.AutoModelForCausalLMWithValueHead.from_pretrained(\"lvwerra/gpt2-imdb\", device_map=device)\n",
    "main_model = peft.get_peft_model(main_model, peft_config, adapter_name='default')\n",
    "main_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIQK5bcpCPZ6"
   },
   "source": [
    "Same as before, trl has a special type of trainer that minimize PPO-specific pseudo-loss. You can read more on this trainer [here](https://huggingface.co/docs/trl/main/en/ppo_trainer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "EvTtiLs94txE"
   },
   "outputs": [],
   "source": [
    "training_args = trl.PPOConfig(\n",
    "    model_name=main_model.config._name_or_path,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=1.41e-5,\n",
    "    batch_size=64,\n",
    "    ppo_epochs=4,                 # PPO performs this many updates per training batch\n",
    "    mini_batch_size=32\n",
    ")\n",
    "\n",
    "ppo_trainer = trl.PPOTrainer(\n",
    "    training_args, model=main_model.model, tokenizer=main_tokenizer,\n",
    "    dataset=imdb_for_rlhf, data_collator=lambda data: dict((key, [d[key] for d in data]) for key in data[0])\n",
    ")  # note: we pass main_model.model because PPOTrainer checks for one of several supported model types ...\n",
    "# ... main_model.model is a model with adapters, which is supported. main_model itself is a wrapper that is not supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "74b0875944e047e6a4d09cc988013ae8",
      "bc859c36e95646b78e9a08111ce1735b",
      "98156b41fab9493cac7eb8edfd1f611a",
      "f0adf0ab77d74ff3857ffa5b9b1a0373",
      "6d3c3f5bfad345f68b6e62ab870e69bf",
      "5cdc9539cca3499abb4958d40142d77c",
      "a984b403d0464e88b4539d586dfc4cc2",
      "23db3f8d31cc4c5d98af465767af45af",
      "4922d11311b846f59278623ec5b6dd39",
      "c487b0154d434955ba8070253af01e1c",
      "c270953012ea437fa8ff299292a41837"
     ]
    },
    "id": "eYr-w666-QfK",
    "outputId": "96206a86-ee25-4e74-a950-f52be855cc24"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c798079a1ef4d38bfa8f9a2f79e20b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 0 ------------------------------\n",
      "rewards/mean:\t0.776145935\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.136003047\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.000000000\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 1 ------------------------------\n",
      "rewards/mean:\t0.557811737\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.075111836\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-0.022935351\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 2 ------------------------------\n",
      "rewards/mean:\t1.544094086\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.280808270\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-0.026567433\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 3 ------------------------------\n",
      "rewards/mean:\t1.521612167\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.319375157\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-0.033960678\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 4 ------------------------------\n",
      "rewards/mean:\t0.810514450\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.090982363\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.000415036\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 5 ------------------------------\n",
      "rewards/mean:\t0.076419830\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.078317910\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-0.009170821\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 6 ------------------------------\n",
      "rewards/mean:\t0.406818390\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.175765246\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-0.092811562\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 7 ------------------------------\n",
      "rewards/mean:\t0.314198971\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.175104499\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-0.015188360\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 8 ------------------------------\n",
      "rewards/mean:\t0.939151764\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.229666531\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-0.063833915\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 9 ------------------------------\n",
      "rewards/mean:\t0.282022953\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.120047428\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-0.065567650\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 10 ------------------------------\n",
      "rewards/mean:\t-0.141558647\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.061861895\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-0.052938066\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 11 ------------------------------\n",
      "rewards/mean:\t0.080347061\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.129513651\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-0.023515878\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 12 ------------------------------\n",
      "rewards/mean:\t0.796915054\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.269411564\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.022267714\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 13 ------------------------------\n",
      "rewards/mean:\t0.945423126\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.255712628\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-0.005292807\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 14 ------------------------------\n",
      "rewards/mean:\t1.248475552\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.426652730\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-0.125251457\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 15 ------------------------------\n",
      "rewards/mean:\t1.015853882\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.315826952\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.135252208\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 16 ------------------------------\n",
      "rewards/mean:\t1.486615658\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.390839219\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.029981855\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 17 ------------------------------\n",
      "rewards/mean:\t2.145724297\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.561865091\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t-0.052157793\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 18 ------------------------------\n",
      "rewards/mean:\t1.310571671\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.492551804\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.302552760\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 19 ------------------------------\n",
      "rewards/mean:\t0.933620453\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.396356761\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.201212108\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 20 ------------------------------\n",
      "rewards/mean:\t1.790081024\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.578175664\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.231429398\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 21 ------------------------------\n",
      "rewards/mean:\t1.533555984\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.592012346\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.407714009\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 22 ------------------------------\n",
      "rewards/mean:\t1.604068756\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.607100785\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.485164821\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 23 ------------------------------\n",
      "rewards/mean:\t2.165227890\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.821359098\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.327856302\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 24 ------------------------------\n",
      "rewards/mean:\t1.651411057\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.751501203\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.663752556\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 25 ------------------------------\n",
      "rewards/mean:\t2.110914230\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.790956199\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.798734963\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 26 ------------------------------\n",
      "rewards/mean:\t1.976063728\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.922854662\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.623931229\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 27 ------------------------------\n",
      "rewards/mean:\t1.960958481\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.956046283\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.769552588\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 28 ------------------------------\n",
      "rewards/mean:\t2.461838722\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.097856522\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.744296551\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 29 ------------------------------\n",
      "rewards/mean:\t1.399875164\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.859034777\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.816996336\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 30 ------------------------------\n",
      "rewards/mean:\t2.383519650\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.161600828\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.341162562\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 31 ------------------------------\n",
      "rewards/mean:\t2.058904648\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.195368052\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.426987767\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 32 ------------------------------\n",
      "rewards/mean:\t1.663520813\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.015666962\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.915063143\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 33 ------------------------------\n",
      "rewards/mean:\t1.980875969\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.188857675\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.887627840\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 34 ------------------------------\n",
      "rewards/mean:\t2.097992420\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.177507877\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.937849760\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 35 ------------------------------\n",
      "rewards/mean:\t2.534427643\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.315840006\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.980741620\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 36 ------------------------------\n",
      "rewards/mean:\t2.441324234\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.375143290\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.339870930\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 37 ------------------------------\n",
      "rewards/mean:\t2.497086525\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.406897545\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.776538253\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 38 ------------------------------\n",
      "rewards/mean:\t2.708084106\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.513789654\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.940876961\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 39 ------------------------------\n",
      "rewards/mean:\t3.131222725\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.639247775\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.791253090\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 40 ------------------------------\n",
      "rewards/mean:\t2.354566574\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.378384352\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.487521887\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 41 ------------------------------\n",
      "rewards/mean:\t2.846868515\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.732823372\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.877988338\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 42 ------------------------------\n",
      "rewards/mean:\t3.157971382\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.804386616\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.998353481\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 43 ------------------------------\n",
      "rewards/mean:\t2.996450424\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.850334644\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.161306381\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 44 ------------------------------\n",
      "rewards/mean:\t2.965475082\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.828534842\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.417247295\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 45 ------------------------------\n",
      "rewards/mean:\t2.169828415\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.586081982\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.125236750\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 46 ------------------------------\n",
      "rewards/mean:\t2.741516113\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.839639425\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.037898540\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 47 ------------------------------\n",
      "rewards/mean:\t2.390049934\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.740243673\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.755356789\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 48 ------------------------------\n",
      "rewards/mean:\t2.962831497\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t2.012710571\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.161844254\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 49 ------------------------------\n",
      "rewards/mean:\t2.585986137\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.884381294\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.776495457\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "max_steps = 50   # can be insufficient for some tasks - watch your learning curves\n",
    "generation_kwargs = dict(\n",
    "    min_length=-1, max_new_tokens=128, do_sample=True, top_k=0, top_p=1.0, pad_token_id=main_tokenizer.eos_token_id)\n",
    "#                                  ^-- task-specific parameter!\n",
    "with tqdm(enumerate(ppo_trainer.dataloader), total=max_steps) as progressbar:\n",
    "  # note: ppo_trainer.dataloader is just a regular dataloader of queries, no RL-specific magic :)\n",
    "  for epoch, batch in progressbar:\n",
    "    if epoch >= max_steps:\n",
    "        break\n",
    "\n",
    "    # Rollout stage: generate continuations from batch queries using main_model\n",
    "    response_tensors = ppo_trainer.generate(batch['input_ids'], **generation_kwargs)\n",
    "    # ^-- list of tensors of token ids from main model tokenizer\n",
    "\n",
    "    # de-tokenize responses to strings (since reward model uses a different tokenizer)\n",
    "    batch[\"response\"] = [main_tokenizer.decode(response.squeeze()) for response in response_tensors]\n",
    "    # note: response_tensors already contain query tokens, so we don't need to add queries manually.\n",
    "    # This may not be true for other tasks: check this manually by viewing batch[\"response\"] and batch[\"query\"]\n",
    "\n",
    "\n",
    "    # Evaluation stage\n",
    "    rewards = compute_reward(batch['response'])\n",
    "\n",
    "    # Update stage\n",
    "    stats = ppo_trainer.step(batch['input_ids'], response_tensors, list(rewards.split(1)))\n",
    "    stats['rewards/mean'] = rewards.mean().item()\n",
    "\n",
    "    print(\"-\" * 30, 'STEP', epoch, '-' * 30)\n",
    "    print(f'rewards/mean:\\t{stats[\"rewards/mean\"]:.9f}\\t<---- average reward over this batch (higher=better, noisy)')\n",
    "    print(f'ppo/returns/mean:\\t{stats[\"ppo/returns/mean\"]:.9f}\\t<---- model-estimated average discounted reward')\n",
    "    print(f'objective/kl:\\t{stats[\"objective/kl\"]:.9f}\\t<---- how far we are from the original model (regularizer)')\n",
    "    print()\n",
    "\n",
    "    ppo_trainer.log_stats(stats, batch, list(rewards.split(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hgtmjtilq6T8"
   },
   "source": [
    "## Main assignment - <u>actually</u> train the model (8 points)\n",
    "\n",
    "\n",
    "Your main task for this week is to use the RLHF pipeline to train a model for a reward of your choice. Here's what you can choose from:\n",
    "\n",
    "__A. Toxicity fine-tuning:__ train the model to be less (or more!) toxic. For this task, you may use the data from [jigsaw toxic comments](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) and [lmsys/toxic-chat](https://huggingface.co/datasets/lmsys/toxic-chat),  or any other source. Alternatively, you may use toxicity scores from [oasst1](https://huggingface.co/datasets/OpenAssistant/oasst1).\n",
    "\n",
    "\n",
    "__B. Actual human feedback:__ use one of the existing datasets with pairwise human feedback to align your langauge model. You may use [anthropic's hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf), [OpenAssistant dataset](https://huggingface.co/datasets/OpenAssistant/oasst1) or any other data you see fit. You may also turn the tables and train the model to [minimize](https://habrastorage.org/getpro/geektimes/post_images/ac7/2ad/827/ac72ad82767d4132164a4b6b76196c42.jpg) human preferences, as long as your model does not degrade to gibberish.\n",
    "\n",
    "__C. Controlled generation:__ Instead of training a reward model from human feedback, you may define the reward function as the text length (longer or shorter) or number of times the model uses specific words (e.g. \"sorry\", \"apologize\"). If you choose specific words, make sure the model generates them at least sometimes.\n",
    "\n",
    "__Alternatively,__ you may choose a different task. However, unless your task is very similar to one of the above, there is a chance that it will be **significantly** harder to solve, requiring orders of magnitude more compute and tuning. If you are in doubt, please ask the course staff. If they are AFK (again >.<), please prefer one of the recommended tasks.\n",
    "\n",
    "\n",
    "#### General tips & tricks\n",
    "\n",
    "\n",
    "Things to look out for:\n",
    "- during PPO stage, the reward model should be in eval mode (dropout disabled)\n",
    "- make sure max_length and max_new_tokens are enough for your chosen dataset - at least most of the time\n",
    "- when in doubt, view the data manually or inspect how the model performs on a few samples\n",
    "\n",
    "\n",
    "We highly recommend that you manually check the performance after each sub-stage:\n",
    "1. when you assembled the pairwise dataset, inspect a couple of from of *your* dataset class and detokenize them. Make sure that you-the-human understand why one sample was accepted and the other - rejected. At least most of the time. This also lets you spot tokenization/truncation errors.\n",
    "2. after you trained a reward model, measure how accurate this model is in isolation. If your reward model is poor, any subsequent RLHF will also fail.\n",
    "3. once you've trained the main model with RL, ask it to generate examples and explore how well it does. If it produces an obviously bad output, check if the reward model assigns high reward to that output. If yes, reward model is the culprit; if no, it's a question of better/longer PPO training.\n",
    "\n",
    "__It is also a good idea to periodically print samples during training.__\n",
    "\n",
    "__When stuck, simplify the problem.__ If you've spent a several hours enchanting the reward model but it still won't budge, try switching to a simple subtask. For instance, if you're training on hh-rlhf, try limiting it the dataset to 10% of the shortest sequences - they are typically easier to learn.\n",
    "\n",
    "\n",
    "## Assignment stages (and grading)\n",
    "\n",
    "Regardless of the specific task you chose, your solution needs to contain several parts that will be graded separately.\n",
    "\n",
    "\n",
    "#### Stage 1: reward model (4 points)\n",
    "\n",
    "Construct a dataset for training the reward model on your problem. Then, train a reward model on that dataset and evaluate how well can your model predict preferences on a hold-out (test) subset of your data.\n",
    "\n",
    "Please make sure that the part of your notebook where you evaluate reward model is clearly visible and reasonably easy to read. And for all that is holy, do not call it IMDB unless it actually **is** data of imdb movie reviews :)\n",
    "\n",
    "__Not all tasks require a reward model for later PPO fine-tuning.__ For instance, there's no reason to train a reward model if your reward equals sentence length. Likewise, toxicity reward can be estimated with a pre-trained toxicity classifier. __If your task does not require training a reward model, please train an unrelated model on [hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf) as though you were solving assignment version B.__ This is for grading purposes only, you won't use this model for stage 2.\n",
    "\n",
    "\n",
    "#### Stage 2: RL fine-tuning (4 points)\n",
    "\n",
    "Once the reward model is ready - or you can compute rewards without a model - it is time to maximize that reward with PPO. Optionally, you may replace PPO with another RL algorithm (or unlikelihood learning scheme), but only if you're feeling adventurous.\n",
    "\n",
    "\n",
    "First, you need to choose a language model to be fine-tuned. You may choose any model, but make sure that your model **can** generate the data in your format. For instance, [Mistral-7B](https://huggingface.co/mistralai/Mistral-7B-v0.1) is a general purpose LM and may (or may not) need prompt engineering to generate chat assistant responses. For that reason, it is best if you **do not use `\"lvwerra/gpt2-imdb\"` unless you're generating only movie reviews**.\n",
    "\n",
    "\n",
    "\n",
    "There are two \"difficulty modes\" for this task:\n",
    "For the **easy mode**, use [gpt2-large](https://huggingface.co/gpt2-large) or [opt-1.3b](https://huggingface.co/facebook/opt-1.3b) with minimal code changes.\n",
    "If you want the **Hard mode:** use a larger (e.g. 7B) model in combination with `load_in_4bit` and LoRA, the same way we did last week.\n",
    "Some reasonable model choices are [LLaMA-7B](https://huggingface.co/Enoch/llama-7b-hf), [Falcon-7b](https://huggingface.co/tiiuae/falcon-7b), [Mistral-7B](https://huggingface.co/mistralai/Mistral-7B-v0.1) for general-purpose LM or [guanaco-7b](https://huggingface.co/timdettmers/guanaco-7b), [vicuna-7b](https://huggingface.co/lmsys/vicuna-7b-v1.5) for chat-based tasks, though there are many more (see [leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)). In the hard mode, you will need to modify the training arguments to enable 4-bit fine-tuning. Furthermore, your experiments will take somewhat longer to complete. On the plus side, your model will produce significantly better results.\n",
    "\n",
    "__High reward is not enough!__ RL algorithms are famous for [cheating their reward functions](https://openai.com/research/faulty-reward-functions). To ensure that your model is actually doing what you want it to do, you will need some additional evaluation. To get the full grade, provide at least 20 side-by-side examples of your fine-tuned model vs original model predictions and a short summary.\n",
    "\n",
    "Alternatively, you may provide 5 examples and some extrinsic evaluation metric over many examples. For instance, you may use a different pre-trained toxicity score for option A. When dealing with human preferences, you may choose to [enlist actual humans](https://toloka.ai/) or [ask GPT4/Claude](https://arxiv.org/pdf/2304.03277.pdf) to compare your model's predictions. For task C, when optimizing for simple rewards like sentence lengths, it is enough to compare histograms of rewards (e.g. average lengths).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Dataset\n",
    "import trl\n",
    "import peft\n",
    "import transformers\n",
    "import torch\n",
    "import random\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# данные\n",
    "# https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data\n",
    "\n",
    "# import zipfile\n",
    "# import os \n",
    "# os.chdir(\".\")\n",
    "\n",
    "# with zipfile.ZipFile(\"data/jigsaw-toxic-comment-classification-challenge.zip\", 'r') as zip_ref:\n",
    "#     zip_ref.extractall(\"data/\")\n",
    "\n",
    "# with zipfile.ZipFile(\"data/train.csv.zip\", 'r') as zip_ref:\n",
    "#     zip_ref.extractall(\"data/\")\n",
    "\n",
    "# with zipfile.ZipFile(\"data/test.csv.zip\", 'r') as zip_ref:\n",
    "#     zip_ref.extractall(\"data/\")\n",
    "\n",
    "# with zipfile.ZipFile(\"data/test_labels.csv.zip\", 'r') as zip_ref:\n",
    "#     zip_ref.extractall(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'],\n",
      "        num_rows: 159571\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'],\n",
      "        num_rows: 63978\n",
      "    })\n",
      "})\n",
      "{'comment_text': '\"\\nAnd ... I really don\\'t think you understand.  I came here and my idea was bad right away.  What kind of community goes \"\"you have bad ideas\"\" go away, instead of helping rewrite them.   \"', 'toxic': 0, 'severe_toxic': 0, 'obscene': 0, 'threat': 0, 'insult': 0, 'identity_hate': 0}\n"
     ]
    }
   ],
   "source": [
    "jigsaw = datasets.load_dataset('jigsaw_toxicity_pred', data_dir='data/')\n",
    "print(jigsaw)\n",
    "print(jigsaw['train'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_equal_toxic(dataset, min_size=7000):\n",
    "    toxic_comments = [row for row in dataset if row['toxic'] == 1]\n",
    "    non_toxic_comments = [row for row in dataset if row['toxic'] == 0]\n",
    "    print(f\"Total toxic comments: {len(toxic_comments)}\")\n",
    "    print(f\"Total non-toxic comments: {len(non_toxic_comments)}\")\n",
    "    \n",
    "    min_size = min(len(toxic_comments), len(non_toxic_comments), min_size)\n",
    "    \n",
    "    balanced_toxic_comments = random.sample(toxic_comments, min_size)\n",
    "    balanced_non_toxic_comments = random.sample(non_toxic_comments, min_size)\n",
    "    \n",
    "    print(f\"Balanced toxic comments: {len(balanced_toxic_comments)}\")\n",
    "    print(f\"Balanced non-toxic comments: {len(balanced_non_toxic_comments)}\")\n",
    "    \n",
    "    balanced_dataset = balanced_toxic_comments + balanced_non_toxic_comments\n",
    "    random.shuffle(balanced_dataset)\n",
    "    \n",
    "    print(f\"Total balanced dataset: {len(balanced_dataset)}\")\n",
    "    return Dataset.from_list(balanced_dataset)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total toxic comments: 15294\n",
      "Total non-toxic comments: 144277\n",
      "Balanced toxic comments: 7000\n",
      "Balanced non-toxic comments: 7000\n",
      "Total balanced dataset: 14000\n",
      "Total toxic comments: 6090\n",
      "Total non-toxic comments: 57888\n",
      "Balanced toxic comments: 6090\n",
      "Balanced non-toxic comments: 6090\n",
      "Total balanced dataset: 12180\n"
     ]
    }
   ],
   "source": [
    "jigsaw['train'] = make_equal_toxic(jigsaw['train'])\n",
    "jigsaw['test'] = make_equal_toxic(jigsaw['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/nikolaev.oleg15/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_model = transformers.AutoModelForSequenceClassification.from_pretrained(\"distilroberta-base\", device_map=device) # albert-base-v2 # distilbert-base-cased # distilroberta-base\n",
    "reward_tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilroberta-base\") \n",
    "reward_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7000 toxic and 7000 non-toxic comments.\n",
      "Found 6090 toxic and 6090 non-toxic comments.\n"
     ]
    }
   ],
   "source": [
    "class JigsawPairwiseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.toxic_comments = [row['comment_text'] for row in dataset if row['toxic'] == 1]\n",
    "        self.non_toxic_comments = [row['comment_text'] for row in dataset if row['toxic'] == 0]\n",
    "        print(f\"Found {len(self.toxic_comments)} toxic and {len(self.non_toxic_comments)} non-toxic comments.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.toxic_comments) * len(self.non_toxic_comments)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        toxic_comments = self.tokenizer(self.toxic_comments[index // len(self.toxic_comments)], truncation=True)\n",
    "        non_toxic_comments = self.tokenizer(self.non_toxic_comments[index % len(self.non_toxic_comments)], truncation=True)\n",
    "        return dict(input_ids_chosen=toxic_comments['input_ids'], attention_mask_chosen=toxic_comments['attention_mask'],\n",
    "                    input_ids_rejected=non_toxic_comments['input_ids'], attention_mask_rejected=non_toxic_comments['attention_mask'])\n",
    "\n",
    "\n",
    "jigsaw_pairwise_train = JigsawPairwiseDataset(jigsaw['train'], reward_tokenizer)\n",
    "jigsaw_pairwise_test = JigsawPairwiseDataset(jigsaw['test'], reward_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic_comments: <s>You Zionist Jewbastard Khazar Turks just love filibusters that draw out this tragedy to no conclusion.  That's right, only YOU are allowed a say on the issue.  YOU have the right to editorialise anything to YOUR content, media mogul jackasses!  Stay out of London, New York, Washington and Hollywood!  Get the fuck out of America and stop dragging us into your stupid affairs with Muslims!  You deserved 9/11 and I hope more of you die from suicide bombings by economically tortured Muslims, just keep it in the Middle East.  Helen Clark did well to not take your shite!  I swear, I'll fucking kill you all if I ever go to Israel.  I'll take nukes signed by each and every Jew of the Manhattan Project and level you to nothing; in a eulogy to Theodore Herzl.  What irony, to die by the products of your own hands, that had me fear for my life in the fucking Cold War.  Mad scientists and loan sharks, fucking trash with no goddamn decency to Europe and America!  Wanderer gypsies, no sense of love for your own people enough to stick together on your own land.  Can't even settle down and do your own thing away from others.  Leave us and Rachel Corrie alone!  You have no respect for the dead!  To you, she was just another Goy puppet!  You will pay and I hope to personally see you die.  Perhaps a Tay-Sachs bio-weapon to plague and infest the lot of you until death do we part.  Take your Michael Medveds, Adam Sandlers and Bob Dylans and shove them up your arses!  No more Disraelis and Kerrys or Rothschilds and Greenspans will ruin our lives!  You bring disrepute to White males, by rewriting history; fucking up churches and schools in culture war.  You killed McKinley and the Romanovs.  Fuck your Sigmund Freud, Ron Jeremy and Ruth Westheimer pervert terrorists and kill yourselves NOW!  You fucking Howard Stern/Jerry Seinfeld/Eugene Levy garbage are no better than any other Semite like the Muslims themselves!  You anti-Semitic hypocrites have no feature but greed and barbarism!  I read the massacre at Clifford's Tower in York Castle and hope for another!  Your ritual child abuse still mutilates baby genitals, just like labia removal. </s>\n",
      "non_toxic_comments: <s>May 2008 (UTC)\n",
      "\n",
      " So anyone who has diagreements with VK in the past is now banned from editing any area he takes an interest in, incase he comes into contact with them?   11:19, 7</s>\n"
     ]
    }
   ],
   "source": [
    "sample = jigsaw_pairwise_train[0]\n",
    "print('toxic_comments:', reward_tokenizer.decode(sample['input_ids_chosen']))\n",
    "print('non_toxic_comments:', reward_tokenizer.decode(sample['input_ids_rejected']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids_chosen': [0, 1185, 34387, 16495, 428, 1988, 1120, 2218, 18692, 27539, 95, 657, 46189, 37406, 14, 2451, 66, 42, 6906, 7, 117, 6427, 4, 1437, 280, 18, 235, 6, 129, 10540, 32, 1220, 10, 224, 15, 5, 696, 4, 1437, 10540, 33, 5, 235, 7, 8161, 1496, 932, 7, 21688, 1383, 6, 433, 18248, 10267, 24473, 328, 1437, 9631, 66, 9, 928, 6, 188, 469, 6, 663, 8, 3049, 328, 1437, 2315, 5, 26536, 66, 9, 730, 8, 912, 19335, 201, 88, 110, 12103, 5185, 19, 6299, 328, 1437, 370, 10973, 361, 73, 1225, 8, 38, 1034, 55, 9, 47, 1597, 31, 4260, 19918, 30, 14738, 20464, 6299, 6, 95, 489, 24, 11, 5, 2367, 953, 4, 1437, 11668, 4433, 222, 157, 7, 45, 185, 110, 1481, 1459, 328, 1437, 38, 24909, 6, 38, 581, 23523, 3549, 47, 70, 114, 38, 655, 213, 7, 1870, 4, 1437, 38, 581, 185, 295, 23369, 1419, 30, 349, 8, 358, 16495, 9, 5, 6562, 3728, 8, 672, 47, 7, 1085, 131, 11, 10, 364, 922, 21370, 7, 26164, 26288, 462, 4, 1437, 653, 21490, 6, 7, 1597, 30, 5, 785, 9, 110, 308, 1420, 6, 14, 56, 162, 2490, 13, 127, 301, 11, 5, 23523, 7810, 1771, 4, 1437, 4145, 4211, 8, 2541, 21278, 6, 23523, 8875, 19, 117, 45003, 30338, 7, 1005, 8, 730, 328, 1437, 20233, 7160, 18124, 3275, 918, 6, 117, 1472, 9, 657, 13, 110, 308, 82, 615, 7, 4757, 561, 15, 110, 308, 1212, 4, 1437, 2615, 75, 190, 6307, 159, 8, 109, 110, 308, 631, 409, 31, 643, 4, 1437, 15084, 201, 8, 7423, 2812, 3636, 1937, 328, 1437, 370, 33, 117, 2098, 13, 5, 1462, 328, 1437, 598, 47, 6, 79, 21, 95, 277, 272, 2160, 29771, 328, 1437, 370, 40, 582, 8, 38, 1034, 7, 5636, 192, 47, 1597, 4, 1437, 6259, 10, 18970, 12, 104, 1488, 29, 10709, 12, 44538, 7, 27936, 8, 4047, 990, 5, 319, 9, 47, 454, 744, 109, 52, 233, 4, 1437, 4624, 110, 988, 5066, 5202, 29, 6, 3086, 4219, 7906, 8, 3045, 211, 4360, 1253, 8, 34940, 106, 62, 110, 4709, 24151, 328, 1437, 440, 55, 6310, 37715, 354, 8, 9153, 29, 50, 33403, 29, 8, 11876, 642, 1253, 40, 17948, 84, 1074, 328, 1437, 370, 836, 2982, 12597, 4467, 7, 735, 14705, 6, 30, 40857, 750, 131, 23523, 62, 10550, 8, 1304, 11, 2040, 997, 4, 1437, 370, 848, 15606, 607, 8, 5, 7733, 1417, 29, 4, 1437, 43774, 110, 208, 35045, 3194, 40469, 6, 5529, 5653, 8, 10871, 580, 17905, 228, 9942, 7263, 8, 3549, 31954, 14279, 328, 1437, 370, 23523, 5218, 21047, 73, 39237, 1608, 35194, 73, 717, 3252, 2552, 20051, 11671, 32, 117, 357, 87, 143, 97, 11202, 1459, 101, 5, 6299, 1235, 328, 1437, 370, 1475, 12, 21004, 44901, 39651, 33, 117, 1905, 53, 26253, 8, 41503, 1809, 328, 1437, 38, 1166, 5, 12998, 23, 20791, 18, 7186, 11, 469, 8834, 8, 1034, 13, 277, 328, 1437, 2486, 19100, 920, 2134, 202, 16119, 718, 1626, 1928, 33079, 6, 95, 101, 6348, 493, 7129, 4, 1437, 2], 'attention_mask_chosen': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids_rejected': [0, 10004, 2266, 36, 41934, 43, 50140, 407, 1268, 54, 34, 2269, 1073, 39510, 19, 42033, 11, 5, 375, 16, 122, 4968, 31, 5390, 143, 443, 37, 1239, 41, 773, 11, 6, 5853, 3175, 37, 606, 88, 1511, 19, 106, 116, 1437, 1437, 365, 35, 1646, 6, 262, 2], 'attention_mask_rejected': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "for i in jigsaw_pairwise_train:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reward_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = trl.RewardConfig(\n",
    "    output_dir=\"reward_model_new\",\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=1.41e-5,\n",
    "    max_steps=1_000,  # Training may need more than 1k steps\n",
    "    logging_steps=50,\n",
    "    gradient_checkpointing=True,  # Reduce memory usage but train ~30% slower\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    fp16=True,  # Disable this on CPU or on very old GPUs\n",
    "    report_to=\"none\",\n",
    "    # max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikolaev.oleg15/.local/lib/python3.10/site-packages/trl/trainer/reward_trainer.py:172: UserWarning: When using RewardDataCollatorWithPadding, you should set `max_length` in RewardConfig. It will be set to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/home/nikolaev.oleg15/.local/lib/python3.10/site-packages/trl/trainer/reward_trainer.py:189: UserWarning: When using RewardDataCollatorWithPadding, you should set `remove_unused_columns=False` in your RewardConfig we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = trl.RewardTrainer(\n",
    "    model=reward_model,\n",
    "    args=training_args,\n",
    "    tokenizer=reward_tokenizer,\n",
    "    train_dataset=jigsaw_pairwise_train,\n",
    "    # eval_dataset=jigsaw_pairwise_test,  # Optional: for evaluation during training\n",
    "    peft_config=None  # Optionally, you may tune with LoRA, prompt-tuning, etc.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikolaev.oleg15/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 03:33, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.068700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.058500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.043600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.047500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.022700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.044900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.036300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.035200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.021100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.036700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.041600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.030700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.027500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.028600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.024300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.035500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.024400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikolaev.oleg15/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.03647275030612945, metrics={'train_runtime': 215.6464, 'train_samples_per_second': 148.391, 'train_steps_per_second': 4.637, 'total_flos': 0.0, 'train_loss': 0.03647275030612945, 'epoch': 0.0006530612244897959})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_model.gradient_checkpointing_disable()\n",
    "reward_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: You could find more details in article WWI and add them here.\n",
      "REWARD: -7.2890625\n",
      "LABEL: 0\n",
      "\n",
      "TEXT: Accept my apology! \n",
      "\n",
      "Accept my apology THIS MINUTE you big, fat noob (this is doughnuthead talking,and if you don't think so, tough.)78.144.87.71\n",
      "REWARD: 6.15625\n",
      "LABEL: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_indices = [45, 12] \n",
    "\n",
    "for sample_index in sample_indices:\n",
    "    text = jigsaw['train'][sample_index]['comment_text']\n",
    "    label = jigsaw['train'][sample_index]['toxic']  # Toxicity label (0 = non-toxic, 1 = toxic)\n",
    "    inputs = reward_tokenizer(text, truncation=True, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        reward = reward_model(**inputs).logits[0, 0].item()\n",
    "    print('TEXT:', text)\n",
    "    print(\"REWARD:\", reward)\n",
    "    print('LABEL:', label)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_roc_auc(reward_model, dataset, tokenizer, device, N=None):\n",
    "    rewards = []\n",
    "    labels = []\n",
    "    if N:\n",
    "        dataset = dataset.shuffle(seed=42)\n",
    "    for n, sample in enumerate(tqdm(dataset)):\n",
    "        inputs = tokenizer(sample['comment_text'], truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            reward = reward_model(**inputs).logits[0, 0].item()\n",
    "        rewards.append(max(0, reward))\n",
    "        labels.append(sample['toxic'])\n",
    "        if N and n >= N:\n",
    "            break\n",
    "    rewards = np.array(rewards)\n",
    "    labels = np.array(labels)\n",
    "    # ROC AUC score\n",
    "    roc_auc = roc_auc_score(labels, rewards)\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb86179539c243a790ef1e5fc80690c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC on training data: 0.9820517857142856\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "roc_auc_train = evaluate_roc_auc(reward_model, jigsaw['train'], reward_tokenizer, device, N=None)\n",
    "print(f\"ROC AUC on training data: {roc_auc_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f207d768bed74cc79ffc568193611b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC on test data: 0.956944909553199\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "roc_auc_test = evaluate_roc_auc(reward_model, jigsaw['test'], reward_tokenizer, device, N=None)\n",
    "print(f\"ROC AUC on test data: {roc_auc_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc0ada5d9da41c2a6bd85cc1013eb8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8cbc249c6e44ff6ad85525e09324d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12180 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_length = trl.core.LengthSampler(2, 8)\n",
    "\n",
    "def select_query_and_tokenize(sample):\n",
    "    query_ids = main_tokenizer.encode(sample[\"comment_text\"])[: sample_length()]\n",
    "    sample[\"query\"] = main_tokenizer.decode(query_ids)  # Query is the only required column\n",
    "    sample[\"input_ids\"] = query_ids  # To avoid re-tokenizing later\n",
    "    return sample\n",
    "\n",
    "jigsaw_train_for_rlhf = jigsaw['train'].map(select_query_and_tokenize, batched=False)\n",
    "jigsaw_test_for_rlhf = jigsaw['test'].map(select_query_and_tokenize, batched=False)\n",
    "\n",
    "jigsaw_train_for_rlhf.set_format(type=\"torch\")\n",
    "jigsaw_test_for_rlhf.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikolaev.oleg15/.local/lib/python3.10/site-packages/trl/models/modeling_base.py:328: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loading_func(filename if not use_safe else safe_filename, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,179,648 || all params: 125,620,225 || trainable%: 0.9391\n"
     ]
    }
   ],
   "source": [
    "peft_config = peft.LoraConfig(\n",
    "    task_type=peft.TaskType.CAUSAL_LM, r=32, lora_alpha=32, lora_dropout=0.0, inference_mode=False\n",
    ")\n",
    "\n",
    "main_tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\") #  lvwerra/gpt2-imdb  # gpt2 # gpt2-large\n",
    "main_tokenizer.pad_token = main_tokenizer.eos_token\n",
    "\n",
    "main_model = trl.AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\", device_map=device)\n",
    "main_model = peft.get_peft_model(main_model, peft_config, adapter_name='default')\n",
    "main_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_model = transformers.AutoModelForCausalLM.from_pretrained(\"gpt2-large\", device_map=device) \n",
    "# main_tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2-large\")\n",
    "# main_tokenizer.pad_token = main_tokenizer.eos_token \n",
    "\n",
    "# peft_config = peft.LoraConfig(\n",
    "#     task_type=peft.TaskType.CAUSAL_LM, r=32, lora_alpha=32, lora_dropout=0.0, inference_mode=False\n",
    "# )\n",
    "# main_model = peft.get_peft_model(main_model, peft_config, adapter_name='default')\n",
    "# main_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(texts):\n",
    "    inputs = reward_tokenizer(texts, truncation=True, padding=True, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = reward_model(**inputs).logits\n",
    "        # Reward is the logit for the \"non-toxic\" class (class 0)\n",
    "        rewards = logits[:, 0]  # Higher reward for non-toxic comments\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = trl.PPOConfig(\n",
    "    model_name=main_model.config._name_or_path,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=1.41e-5,\n",
    "    batch_size=64,\n",
    "    ppo_epochs=4,                 # PPO performs this many updates per training batch\n",
    "    mini_batch_size=32\n",
    ")\n",
    "\n",
    "ppo_trainer = trl.PPOTrainer(\n",
    "    training_args, model=main_model.model, tokenizer=main_tokenizer,\n",
    "    dataset=jigsaw_train_for_rlhf, data_collator=lambda data: dict((key, [d[key] for d in data]) for key in data[0])\n",
    ")  # note: we pass main_model.model because PPOTrainer checks for one of several supported model types ...\n",
    "# ... main_model.model is a model with adapters, which is supported. main_model itself is a wrapper that is not supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25778fa10ffb4137962b5b36099ed23f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 0 ------------------------------\n",
      "rewards/mean:\t-1.932178497\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-8.876640320\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.298173904\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 1 ------------------------------\n",
      "rewards/mean:\t-2.744643211\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-9.073869705\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.629736185\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 2 ------------------------------\n",
      "rewards/mean:\t-2.439382553\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-9.036275864\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.043960094\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 3 ------------------------------\n",
      "rewards/mean:\t-2.412654877\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-8.900870323\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.233530521\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 4 ------------------------------\n",
      "rewards/mean:\t-3.459609985\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-9.096353531\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.719192028\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 5 ------------------------------\n",
      "rewards/mean:\t-2.244674683\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-8.784849167\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.114366293\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 6 ------------------------------\n",
      "rewards/mean:\t-2.904404640\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-8.980967522\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.657404423\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 7 ------------------------------\n",
      "rewards/mean:\t-1.206073046\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-8.550199509\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.135324478\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 8 ------------------------------\n",
      "rewards/mean:\t-2.290374756\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-8.698115349\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.990430355\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 9 ------------------------------\n",
      "rewards/mean:\t-2.091327667\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-8.463649750\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.889211178\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 10 ------------------------------\n",
      "rewards/mean:\t-2.605112076\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-8.470924377\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t5.539494991\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 11 ------------------------------\n",
      "rewards/mean:\t-3.119182587\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-8.653377533\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t6.430138588\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 12 ------------------------------\n",
      "rewards/mean:\t-1.689679623\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-8.436332703\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.283370018\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 13 ------------------------------\n",
      "rewards/mean:\t-2.219824076\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-8.375220299\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t7.978678703\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 14 ------------------------------\n",
      "rewards/mean:\t-2.286353588\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-8.466448784\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.771097183\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 15 ------------------------------\n",
      "rewards/mean:\t-1.777980328\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-8.164840698\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t10.231274605\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 16 ------------------------------\n",
      "rewards/mean:\t-1.976554871\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-7.945402145\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t11.623165131\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 17 ------------------------------\n",
      "rewards/mean:\t-2.781224728\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-8.176009178\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t12.001185417\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 18 ------------------------------\n",
      "rewards/mean:\t-1.220855713\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-7.855610371\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t12.514579773\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 19 ------------------------------\n",
      "rewards/mean:\t-2.899405956\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-8.008277893\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t14.058496475\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 20 ------------------------------\n",
      "rewards/mean:\t-1.382125854\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-7.771495819\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t15.103366852\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 21 ------------------------------\n",
      "rewards/mean:\t-0.713844299\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-7.698209286\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t17.907917023\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 22 ------------------------------\n",
      "rewards/mean:\t-1.182616234\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-7.596308708\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t18.351474762\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 23 ------------------------------\n",
      "rewards/mean:\t-2.142295599\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-7.367859840\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t15.888043404\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 24 ------------------------------\n",
      "rewards/mean:\t-3.326671600\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-7.593029499\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t18.626529694\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 25 ------------------------------\n",
      "rewards/mean:\t-1.612669468\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-7.275473595\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t20.177387238\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 26 ------------------------------\n",
      "rewards/mean:\t-1.484680176\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-7.215003967\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t24.557275772\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 27 ------------------------------\n",
      "rewards/mean:\t-2.255273819\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-7.150020599\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t23.030181885\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 28 ------------------------------\n",
      "rewards/mean:\t-1.357639313\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-6.956406593\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t27.805877686\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 29 ------------------------------\n",
      "rewards/mean:\t-0.727778435\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-6.978514194\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t29.650341034\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 30 ------------------------------\n",
      "rewards/mean:\t-1.161951065\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-6.938238144\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t27.985050201\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 31 ------------------------------\n",
      "rewards/mean:\t-1.459703445\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-6.664293289\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t29.696455002\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 32 ------------------------------\n",
      "rewards/mean:\t-1.767520905\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-6.612642288\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t28.600296021\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 33 ------------------------------\n",
      "rewards/mean:\t-2.224933624\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-6.691989899\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t30.856922150\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 34 ------------------------------\n",
      "rewards/mean:\t-2.041852951\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-6.834221363\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t32.009708405\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 35 ------------------------------\n",
      "rewards/mean:\t-2.305780411\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-6.720522404\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t32.330940247\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 36 ------------------------------\n",
      "rewards/mean:\t-2.399973392\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-6.821346283\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t31.740386963\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 37 ------------------------------\n",
      "rewards/mean:\t-1.986949921\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-6.777576447\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t33.770553589\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 38 ------------------------------\n",
      "rewards/mean:\t-2.027351379\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-6.847038269\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t36.110134125\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 39 ------------------------------\n",
      "rewards/mean:\t-2.719944000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-7.079151154\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t38.535755157\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 40 ------------------------------\n",
      "rewards/mean:\t-2.205312729\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-7.004488945\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t33.564941406\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 41 ------------------------------\n",
      "rewards/mean:\t-1.733057022\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-6.909981728\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t35.246746063\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 42 ------------------------------\n",
      "rewards/mean:\t-3.021730423\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-7.130643845\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t39.509883881\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 43 ------------------------------\n",
      "rewards/mean:\t-2.202779770\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-6.936595917\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t34.988655090\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 44 ------------------------------\n",
      "rewards/mean:\t-3.274414062\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-7.385779858\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t38.008735657\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 45 ------------------------------\n",
      "rewards/mean:\t-2.133329391\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-7.210954666\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t35.349040985\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 46 ------------------------------\n",
      "rewards/mean:\t-1.908747673\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-7.242205143\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t37.714179993\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 47 ------------------------------\n",
      "rewards/mean:\t-0.986039639\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-7.007365704\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t38.172599792\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 48 ------------------------------\n",
      "rewards/mean:\t-2.490996838\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-7.144698620\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t35.228794098\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 49 ------------------------------\n",
      "rewards/mean:\t-2.126831532\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-6.911081314\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t32.704132080\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_steps = 50  # training steps\n",
    "generation_kwargs = dict(\n",
    "    min_length=-1, max_new_tokens=128, do_sample=True, top_k=0, top_p=1.0, pad_token_id=main_tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "with tqdm(enumerate(ppo_trainer.dataloader), total=max_steps) as progressbar:\n",
    "    for epoch, batch in progressbar:\n",
    "        if epoch >= max_steps:\n",
    "            break\n",
    "\n",
    "        # Rollout stage: Generate continuations from batch queries\n",
    "        response_tensors = ppo_trainer.generate(batch['input_ids'], **generation_kwargs)\n",
    "\n",
    "        # Decode responses to strings\n",
    "        batch[\"response\"] = [main_tokenizer.decode(response.squeeze()) for response in response_tensors]\n",
    "\n",
    "        # Evaluation stage: Compute rewards\n",
    "        rewards = compute_reward(batch['response'])\n",
    "\n",
    "        # Update stage: Perform PPO update\n",
    "        stats = ppo_trainer.step(batch['input_ids'], response_tensors, list(rewards.split(1)))\n",
    "        stats['rewards/mean'] = rewards.mean().item()\n",
    "\n",
    "        # Log training statistics\n",
    "        print(\"-\" * 30, 'STEP', epoch, '-' * 30)\n",
    "        print(f'rewards/mean:\\t{stats[\"rewards/mean\"]:.9f}\\t<---- average reward over this batch (higher=better, noisy)')\n",
    "        print(f'ppo/returns/mean:\\t{stats[\"ppo/returns/mean\"]:.9f}\\t<---- model-estimated average discounted reward')\n",
    "        print(f'objective/kl:\\t{stats[\"objective/kl\"]:.9f}\\t<---- how far we are from the original model (regularizer)')\n",
    "        print()\n",
    "\n",
    "        ppo_trainer.log_stats(stats, batch, list(rewards.split(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "dummy_generation_config = GenerationConfig()\n",
    "main_model.base_model.generation_config = dummy_generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_guided_generation(prompt, num_samples=16, max_new_tokens=50):\n",
    "    inputs = main_tokenizer([prompt] * num_samples, return_tensors='pt').to(device)\n",
    "    generated_ids = main_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_k=50,  # diversity\n",
    "        top_p=0.95,  # diversity\n",
    "        temperature=0.7  # creativity\n",
    "    )\n",
    "    samples = [main_tokenizer.decode(ids, skip_special_tokens=True) for ids in generated_ids]\n",
    "    rewards = []\n",
    "    for sample in samples:\n",
    "        sample_inputs = reward_tokenizer(sample, truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            reward = reward_model(**sample_inputs).logits[0, 0].item()\n",
    "        rewards.append(reward)\n",
    "    best_sample_index = np.argmax(rewards)\n",
    "    best_sample = samples[best_sample_index]\n",
    "    best_reward = rewards[best_sample_index]\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Best Sample: {best_sample}\")\n",
    "    print(f\"Best Reward: {best_reward:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    return best_sample, best_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: This boy was\n",
      "Best Sample: This boy was a \"lone wolf,\" according to his mother.\n",
      "\n",
      "In early April, her son walked with his mother to the hospital. He was taken to the hospital. He was then taken to the hospital.\n",
      "\n",
      "The following day, the mother\n",
      "Best Reward: 2.4297\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The idea was\n",
      "Best Sample: The idea was that an electrical current could be used to force the body to its desired point. The idea of this would be that the body would be electrically moved by force of gravity. That was the idea of a wire is electrically moved by force of gravity\n",
      "Best Reward: -4.4336\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: I think the NLP course is\n",
      "Best Sample: I think the NLP course is a really the best way to go for this course. It's the most fun to get it together. You're really, really in a very good situation. You're really, really in a very good situation.\n",
      "\n",
      "\n",
      "The NLP course\n",
      "Best Reward: -7.0156\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: You clever as\n",
      "Best Sample: You clever as hell, we can't be wrong. We are trying to help.\"\n",
      "\n",
      "The man went on to say he was \"doing the right thing\" in a statement he gave to the New York Daily News. \"I am in no way trying to\n",
      "Best Reward: 3.1602\n",
      "--------------------------------------------------\n",
      "Prompt: You stupid as\n",
      "Best Sample: You stupid asian fuck. It's like you want to be a real fucking person and you're going to do whatever you want with your life.\n",
      "\n",
      "But you're going to do whatever you want with your life, so you're gonna be able to say\n",
      "Best Reward: 7.5234\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"This boy was\",\n",
    "    \"The idea was\",\n",
    "    \"I think the NLP course is\",\n",
    "    \"You clever as\",\n",
    "    \"You stupid as\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    reward_guided_generation(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение\n",
    "\n",
    "Сравним с промптами модели до и после RLHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"This boy was\",\n",
    "    \"The idea was\",\n",
    "    \"I think the NLP course is\",\n",
    "    \"You clever as\",\n",
    "    \"You stupid as\",\n",
    "    \"I you\",\n",
    "    \"What\",\n",
    "    \"You should\",\n",
    "    \"Plz make\",\n",
    "    \"I whant\",\n",
    "    \"We must create\",\n",
    "    \"How are you\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITH RLHF:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: This boy was\n",
      "Best Sample: This boy was a bad boy. He didn't have a lot of money. He was a bad boy.\n",
      "\n",
      "\"I was a bad boy. I didn't have a lot of money. I was a bad boy. I was a bad boy. I\n",
      "Best Reward: 4.4727\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITHOUT RLHF: This boy was born to white people.\"\n",
      "\n",
      "\"I'd like the man to be a boy or a girl, I'm not going to go away from it.\"\n",
      "\n",
      "My mother was white. The only difference was she'd be a girl. So that\n",
      "\n",
      "WITH RLHF:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The idea was\n",
      "Best Sample: The idea was that he and his sister would use the power of their own will to make their own life. They would see a new life and a new world, and they would build their own new lives.\n",
      "\n",
      "The idea was that they would use all power\n",
      "Best Reward: -3.7832\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITHOUT RLHF: The idea was to see if you could write code using Scala.\n",
      "\n",
      "On the second day Scala was the default runtime. It was supposed to be the default, so everyone could write tests. Even if you knew not to, we had to try it out because\n",
      "\n",
      "WITH RLHF:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: I think the NLP course is\n",
      "Best Sample: I think the NLP course is a good way to get your mind off the book. It is easy to learn. You can find the books that make you feel like you have something in your life. It's the same book you read. You can find books that make you feel\n",
      "Best Reward: -4.8203\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITHOUT RLHF: I think the NLP course is probably the reason.\"\n",
      "\n",
      "However, the current system of government has failed in some areas, which have created pressure to re-impose limits on government spending and increase spending while decreasing freedom to innovate and innovate in the 21st century, said the\n",
      "\n",
      "WITH RLHF:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: You clever as\n",
      "Best Sample: You clever as hell.\"\n",
      "(*)*)\n",
      "\n",
      "(Weas.\n",
      "Best Reward: 6.9531\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITHOUT RLHF: You clever as it may seem, that's not really your fault.\n",
      "\n",
      "It's not the fault of our children, who are too old to be in their late 20s, or of children who are not yet adults yet, because those kids are also not\n",
      "\n",
      "WITH RLHF:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: You stupid as\n",
      "Best Sample: You stupid as fuck, you're an idiot for having to be an idiot. The only thing you've done to make me a better. And I'm a fucking cunt. If you're ever gonna say anything. I'm gonna try to. I'm gonna try\n",
      "Best Reward: 7.5273\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITHOUT RLHF: You stupid as shit. Oh yeah. Here's to a game. That's just what this is all about, isn't it? For one. And now we have a show this little girl named Missy who's played all the girl's best songs and played\n",
      "\n",
      "WITH RLHF:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: I you\n",
      "Best Sample: I you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you\n",
      "Best Reward: 6.1992\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITHOUT RLHF: I you have the right to be in the country if you desire, so long as I am there, and not in Canada\" (A-1135-8, C-1244-A, A A-14). The Constitution, like the Declaration\n",
      "\n",
      "WITH RLHF:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What\n",
      "Best Sample: What's so good about your life? You're a genius! The life you want to live, the life you want to live, the life you want to live, the life you want to live. You're a genius!\n",
      "\n",
      "You're a\n",
      "Best Reward: 2.3301\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITHOUT RLHF: What the hell is up with that? When I saw her in a picture I thought she looked pretty hot. But then her legs are like my pussy.\"\n",
      "\n",
      "When asked why her boyfriend would have done this, Yuki responded, \"Yuki just\n",
      "\n",
      "WITH RLHF:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: You should\n",
      "Best Sample: You should get your own hand off your own hand and get your own hands on, for your own hands, my hand, my hand on my hands, that my hand, and on my hand on my hand, my hand my hand, my hand on my\n",
      "Best Reward: 4.0430\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITHOUT RLHF: You should be able to do this,\" She says, \"and then you can talk about some of these projects by the time you get to college.\"\n",
      "\n",
      "\n",
      "In addition to the big picture issues, the students of this course do have to deal with the many\n",
      "\n",
      "WITH RLHF:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Plz make\n",
      "Best Sample: Plz make a part of my life. You make me.\n",
      "Best Reward: -0.6689\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITHOUT RLHF: Plz make it a few clicks. I am willing to believe them!\n",
      "\n",
      "Now to the last chapter from the end.\n",
      "\n",
      "Part 2:\n",
      "\n",
      "\"So we'll get the whole gang off the boat. Leave this ship and go back to the\n",
      "\n",
      "WITH RLHF:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: I whant\n",
      "Best Sample: I whant is that he can no longer be a fag, he is a man and a man is a man,\"\n",
      "\n",
      "and he shall make a man a man, and he shall make a man\n",
      "\n",
      "and his man\n",
      "\n",
      "and he shall make\n",
      "Best Reward: 6.8047\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITHOUT RLHF: I whant' I be like, hey?\n",
      "\n",
      "[20:01:02 AM] Ian Cheong: I dont think I like that.\n",
      "\n",
      "[20:01:08 AM] Izzy (@iglvzx): Yes, but I dont read the\n",
      "\n",
      "WITH RLHF:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: We must create\n",
      "Best Sample: We must create new jobs and create new jobs.\n",
      "\n",
      "For the last two years, the we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we,\n",
      "Best Reward: 0.9155\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITHOUT RLHF: We must create new products and services in order to continue to attract foreign investment.\n",
      "\n",
      "1.2. Product Delivery Service Providers (PDS) in the UAE\n",
      "\n",
      "PDS, which will be located at various locations in the UAE, also operates an\n",
      "\n",
      "WITH RLHF:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How are you\n",
      "Best Sample: How are you going to look at the whole thing? Why is there this shit, and why I don't have a life of love. Oh, and what I know is why I don't have a. This is a, a.a.a.a\n",
      "Best Reward: 3.9766\n",
      "--------------------------------------------------\n",
      "\n",
      "WITHOUT RLHF: How are you doing with your new project?\n",
      "\n",
      "I'm trying to keep my focus and not fall over myself. As soon as I finish writing the code, I'm going to get up and take a breather and then I'll start going off to training\n"
     ]
    }
   ],
   "source": [
    "not_trained_main_tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "not_trained_main_model = transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\", device_map=device)\n",
    "\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(\"\\nWITH RLHF:\")\n",
    "    reward_guided_generation(prompt)\n",
    "    inputs = not_trained_main_tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    generated_ids = not_trained_main_model.generate(**inputs, max_new_tokens=50, do_sample=True)\n",
    "    print(\"\\nWITHOUT RLHF:\", not_trained_main_tokenizer.decode(generated_ids.flatten().cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Заключение\n",
    "\n",
    "`You clever as...` --> `You clever as hell, we can't be wrong. We are trying to help` - вот так модель продолжила сообщение. Машина по созданию токсичных комментариев готова.\n",
    "\n",
    " В работе было произведено два RLHF алаймента, оценены метрики и сгенирированы примеры работы:\n",
    " 1) `IMBD` датасет, `distilbert-base-cased` в качестве reward_model и `lvwerra/gpt2-imdb` в качестве основной модели\n",
    " 2) `jigsaw toxicity` датасет, `distilroberta-base` в качестве reward_model и `gpt2` в качестве основной модели\n",
    "\n",
    "Из раздела со сравнением, видно как RLHF повлиял на модель (gpt-2) - она теперь создает токсичные предложения. Также, ниже представлены метрики.\n",
    "\n",
    "|  RLHF алаймент     | ROC AUC |\n",
    "| ---------------- |  ---------- |\n",
    "| IMBD            |  0.97 (TRAIN), 0.95 (TEST)   |\n",
    "| jigsaw toxicity               | 0.98 (TRAIN), 0.96 (TEST)    |\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c8ff454cd947027f86954d72bf940c689a97dcc494eb53cfe4813862c6065fe"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1692cc1532df4c2695c729a964a31da8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23db3f8d31cc4c5d98af465767af45af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ca9e640d5d549658e42fd73af8ea399": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "411cd47238a94edc8283e178e04d386f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "44f3d7c434354a90bfa3d4750048b80f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4922d11311b846f59278623ec5b6dd39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4ad4da252cb64e818d35eb3869adc81c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1692cc1532df4c2695c729a964a31da8",
      "max": 24895,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_55c624445ca44090a2f109d3bf5f3f6a",
      "value": 24895
     }
    },
    "55c624445ca44090a2f109d3bf5f3f6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5cdc9539cca3499abb4958d40142d77c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63e252cfd9f44ef79137473b618828dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ca9e640d5d549658e42fd73af8ea399",
      "placeholder": "​",
      "style": "IPY_MODEL_c8ea1c2d3b5040378d0f2bd213933603",
      "value": "Map: 100%"
     }
    },
    "6d3c3f5bfad345f68b6e62ab870e69bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74b0875944e047e6a4d09cc988013ae8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bc859c36e95646b78e9a08111ce1735b",
       "IPY_MODEL_98156b41fab9493cac7eb8edfd1f611a",
       "IPY_MODEL_f0adf0ab77d74ff3857ffa5b9b1a0373"
      ],
      "layout": "IPY_MODEL_6d3c3f5bfad345f68b6e62ab870e69bf"
     }
    },
    "9604bff7c9414071a55d063fdf4174fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "98156b41fab9493cac7eb8edfd1f611a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23db3f8d31cc4c5d98af465767af45af",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4922d11311b846f59278623ec5b6dd39",
      "value": 39
     }
    },
    "a984b403d0464e88b4539d586dfc4cc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc859c36e95646b78e9a08111ce1735b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5cdc9539cca3499abb4958d40142d77c",
      "placeholder": "​",
      "style": "IPY_MODEL_a984b403d0464e88b4539d586dfc4cc2",
      "value": " 78%"
     }
    },
    "c270953012ea437fa8ff299292a41837": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c487b0154d434955ba8070253af01e1c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8ea1c2d3b5040378d0f2bd213933603": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d3a8dcfad53b4de885b39ee83e6029ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_411cd47238a94edc8283e178e04d386f",
      "placeholder": "​",
      "style": "IPY_MODEL_9604bff7c9414071a55d063fdf4174fa",
      "value": " 24895/24895 [00:43&lt;00:00, 568.24 examples/s]"
     }
    },
    "f0adf0ab77d74ff3857ffa5b9b1a0373": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c487b0154d434955ba8070253af01e1c",
      "placeholder": "​",
      "style": "IPY_MODEL_c270953012ea437fa8ff299292a41837",
      "value": " 39/50 [51:09&lt;14:44, 80.38s/it]"
     }
    },
    "f6ba50eafdeb4b94a1e7c22c5027f709": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_63e252cfd9f44ef79137473b618828dd",
       "IPY_MODEL_4ad4da252cb64e818d35eb3869adc81c",
       "IPY_MODEL_d3a8dcfad53b4de885b39ee83e6029ef"
      ],
      "layout": "IPY_MODEL_44f3d7c434354a90bfa3d4750048b80f"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
